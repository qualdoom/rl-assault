{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install gym[atari]\n",
    "# !pip uninstall ale-py\n",
    "# !pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config.config2_ddqn import config as config2\n",
    "from Config.config4_ddqn import config as config4\n",
    "\n",
    "from Config.dueling_config1 import config as dueling_config1\n",
    "from Config.dueling_config2 import config as dueling_config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_lives = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        reward = 0\n",
    "        t = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQNAgent import DQNAgent\n",
    "import DuelingDQNAgent\n",
    "from Environment import PreprocessAtariObs, make_env\n",
    "\n",
    "_env = make_env(seed=2356, skip=4)\n",
    "state_shape = _env.observation_space.shape\n",
    "n_actions = _env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn_exp():\n",
    "    checkpoint = torch.load(config2['best_file_name'] , map_location=torch.device(device))\n",
    "    agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dqn_pr():\n",
    "    checkpoint = torch.load(config4['best_file_name'] , map_location=torch.device(device))\n",
    "    agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dueling_dqn_exp():\n",
    "    checkpoint = torch.load(dueling_config2['best_file_name'] , map_location=torch.device(device))\n",
    "    agent = DuelingDQNAgent.DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dueling_dqn_pr():\n",
    "    checkpoint = torch.load(dueling_config1['best_file_name'] , map_location=torch.device(device))\n",
    "    agent = DuelingDQNAgent.DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_size 1024\n",
      "fc_size 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinik\\Githubik\\ULTRA_SUPER_MEGA_POWER_v2_zxc\\DQNAgent.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  states = torch.tensor(states, device=model_device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m dueling_dqn_pr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([eval_dueling_dqn_pr() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n\u001b[0;32m      3\u001b[0m dqn_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([eval_dqn_exp() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n\u001b[1;32m----> 4\u001b[0m dueling_dqn_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([eval_dueling_dqn_exp() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m dueling_dqn_pr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([eval_dueling_dqn_pr() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n\u001b[0;32m      3\u001b[0m dqn_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([eval_dqn_exp() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n\u001b[1;32m----> 4\u001b[0m dueling_dqn_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43meval_dueling_dqn_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36meval_dueling_dqn_exp\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m agent\u001b[38;5;241m.\u001b[39mtarget_network\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_network\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, clip_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_lives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(env, agent, n_games, greedy, t_max)\u001b[0m\n\u001b[0;32m      8\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t_max):\n\u001b[1;32m---> 10\u001b[0m     qvalues \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m qvalues\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m greedy \u001b[38;5;28;01melse\u001b[39;00m agent\u001b[38;5;241m.\u001b[39msample_actions(qvalues)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m     s, r, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\vinik\\Githubik\\ULTRA_SUPER_MEGA_POWER_v2_zxc\\DuelingDQNAgent.py:29\u001b[0m, in \u001b[0;36mDQNAgent.get_qvalues\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m     27\u001b[0m model_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     28\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(states, device\u001b[38;5;241m=\u001b[39mmodel_device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 29\u001b[0m qvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qvalues\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vinik\\Githubik\\ULTRA_SUPER_MEGA_POWER_v2_zxc\\DuelingNetwork.py:75\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[1;34m(self, state_t)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mtakes agent's observation (tensor), returns qvalues (tensor)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m:param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature(state_t)\n\u001b[1;32m---> 75\u001b[0m advantage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvantage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m value     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[0;32m     77\u001b[0m advantage_average \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(advantage, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vinik\\Githubik\\ULTRA_SUPER_MEGA_POWER_v2_zxc\\DuelingNetwork.py:54\u001b[0m, in \u001b[0;36mNetwork.advantage\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvantage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_advantage(x)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn_pr = np.asarray([eval_dqn_pr() for i in range(10)])\n",
    "dueling_dqn_pr = np.asarray([eval_dueling_dqn_pr() for i in range(10)])\n",
    "dqn_exp = np.asarray([eval_dqn_exp() for i in range(10)])\n",
    "dueling_dqn_exp = np.asarray([eval_dueling_dqn_exp() for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_dqn_pr : 5226.3\n",
      "mean_dueling_dqn_pr : 3559.8\n",
      "mean_dqn_exp : 5680.1\n",
      "mean_dueling_dqn_exp : 3965.0\n"
     ]
    }
   ],
   "source": [
    "print(\"mean_dqn_pr :\", np.mean(dqn_pr))\n",
    "print(\"mean_dueling_dqn_pr :\", np.mean(dueling_dqn_pr))\n",
    "print(\"mean_dqn_exp :\", np.mean(dqn_exp))\n",
    "print(\"mean_dueling_dqn_exp :\", np.mean(dueling_dqn_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_dqn_pr : 8276.0\n",
      "max_dueling_dqn_pr : 5436.0\n",
      "max_dqn_exp : 6723.0\n",
      "max_dueling_dqn_exp : 7187.0\n"
     ]
    }
   ],
   "source": [
    "print(\"max_dqn_pr :\", np.max(dqn_pr))\n",
    "print(\"max_dueling_dqn_pr :\", np.max(dueling_dqn_pr))\n",
    "print(\"max_dqn_exp :\", np.max(dqn_exp))\n",
    "print(\"max_dueling_dqn_exp :\", np.max(dueling_dqn_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random(env, n_games=1, t_max=1000000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        reward = 0\n",
    "        t = 0\n",
    "        for _ in range(t_max):\n",
    "            action = env.action_space.sample()\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random mean : 233.1\n",
      "random max : 399.0\n"
     ]
    }
   ],
   "source": [
    "random_reward = [evaluate_random(make_env(skip=1, clip_rewards=False, render_mode=\"rgb_array\", seed=2127), n_lives) for i in range(10)]\n",
    "\n",
    "print(\"random mean :\", np.mean(random_reward))\n",
    "print(\"random max :\", np.max(random_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
